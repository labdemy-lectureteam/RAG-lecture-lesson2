{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat機能の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .envファイルからAPIキーを呼び出す。\n",
    "\n",
    "st.title('RAG チャットボット')\n",
    "\n",
    "def reset_st_session():\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    st.session_state.model = ChatOpenAI(model='gpt-4o-mini') \n",
    "\n",
    "if \"session_reset_done\" not in st.session_state.keys():\n",
    "    reset_st_session()\n",
    "    st.session_state.session_reset_done = True\n",
    "    \n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message['role']):\n",
    "        st.markdown(message['content'])\n",
    "        \n",
    "user_prompt = st.chat_input()\n",
    "if user_prompt:\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(user_prompt)\n",
    "    st.session_state.messages.append({'role':'user', 'content':user_prompt})\n",
    "    \n",
    "    with st.chat_message('assistant'):\n",
    "        response = st.write_stream(st.session_state.model.stream(user_prompt))\n",
    "    st.session_state.messages.append({'role':'assistant', 'content':response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会話履歴を用いたChatの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変更点\n",
    "- `format_prompt()`関数の定義\n",
    "- `format_prompt()`関数を用いたuser promptの整形とLLMへの入力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "\n",
    "load_dotenv()  # .envファイルからAPIキーを呼び出す。\n",
    "\n",
    "st.title('RAG チャットボット')\n",
    "\n",
    "def reset_st_session():\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    st.session_state.model = ChatOpenAI(model='gpt-4o-mini') \n",
    "\n",
    "if \"session_reset_done\" not in st.session_state.keys():\n",
    "    reset_st_session()\n",
    "    st.session_state.session_reset_done = True\n",
    "    \n",
    "# prompt整形関数の定義\n",
    "def format_prompt(query:str, chat_history:List[Dict[str, str]])->str:\n",
    "    PROMPT = \"\"\"\n",
    "    You are a helpful assistant. Answer the user given the chat history:\n",
    "    \n",
    "    chat history: {CHAT_HISTORY}\n",
    "    user query: {QUERY}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(PROMPT)\n",
    "    chat_history = '\\n\\n'.join([f\"{message['role']}: {message['content']}\" for message in chat_history])\n",
    "    prompt = prompt.format(CHAT_HISTORY=chat_history,QUERY=query)\n",
    "    return prompt\n",
    "\n",
    "    \n",
    "    \n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message['role']):\n",
    "        st.markdown(message['content'])\n",
    "        \n",
    "user_prompt = st.chat_input()\n",
    "if user_prompt:\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(user_prompt)\n",
    "    st.session_state.messages.append({'role':'user', 'content':user_prompt})\n",
    "    \n",
    "    # ユーザーpromptの整形\n",
    "    final_prompt = format_prompt(user_prompt,st.session_state.messages)\n",
    "    \n",
    "    with st.chat_message('assistant'):\n",
    "        # 整形されたpromptの入力\n",
    "        response = st.write_stream(st.session_state.model.stream(final_prompt))\n",
    "    st.session_state.messages.append({'role':'assistant', 'content':response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイルの管理機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from app import reset_st_session\n",
    "\n",
    "st.write('## ファイル管理画面')\n",
    "\n",
    "if uploaded_files := st.file_uploader(\"ファイルをアップロード：\",accept_multiple_files=True):\n",
    "    if not os.path.exists(\"documents\"):\n",
    "        os.makedirs(\"documents\")\n",
    "    for uploaded_file in uploaded_files:\n",
    "        with open(os.path.join(\"documents\",uploaded_file.name),\"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "    reset_st_session()    \n",
    "    msg = 'PDFデータの更新が完了しました。'\n",
    "    st.session_state.messages.append({'role':'assistant', 'content':msg})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルの削除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変更点\n",
    "- ファイルの一覧の追加\n",
    "- 選択したファイルの削除機能の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from app import reset_st_session\n",
    "\n",
    "st.write('## ファイル管理画面')\n",
    "\n",
    "if uploaded_files := st.file_uploader(\"ファイルをアップロード：\",accept_multiple_files=True):\n",
    "    if not os.path.exists(\"documents\"):\n",
    "        os.makedirs(\"documents\")\n",
    "    for uploaded_file in uploaded_files:\n",
    "        with open(os.path.join(\"documents\",uploaded_file.name),\"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "    reset_st_session()    \n",
    "    msg = 'PDFデータの更新が完了しました。'\n",
    "    st.session_state.messages.append({'role':'assistant', 'content':msg})\n",
    "\n",
    "\n",
    "st.write('#### ファイル一覧')\n",
    "files_to_remove = []\n",
    "if os.path.isdir('documents') and len(os.listdir('documents')) != 0:\n",
    "    for file_name in os.listdir('documents'):\n",
    "        remove_file = st.checkbox(file_name)\n",
    "        if remove_file:\n",
    "            files_to_remove.append(file_name)\n",
    "\n",
    "if st.button(\"選択したファイルを削除\",key='delete_files'):\n",
    "    for file in files_to_remove:\n",
    "        os.remove(f'documents/{file}')\n",
    "    st.rerun()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFファイルの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import shutil\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import chromadb\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def load_PDF(path: str) -> List[Document]:\n",
    "    if os.path.isdir(path):\n",
    "        loader = PyPDFDirectoryLoader(path, glob=\"*.pdf\")\n",
    "        documents = loader.load()\n",
    "    elif os.path.isfile(path):\n",
    "        if not path.lower().endswith('.pdf'):\n",
    "            raise ValueError(f\"与えられたファイル：  '{path}' はPDFではありません.\")\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load()\n",
    "    else:\n",
    "        raise ValueError(f\"与えられたパス： '{path}' はファイルでもディレクトリでもありません。\")\n",
    "    if not documents:\n",
    "        raise ValueError(f\"与えられたパス： '{path}' からのファイルの読み込みに失敗しました。\")\n",
    "    return documents\n",
    "\n",
    "def create_chunks(documents: List[Document], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def init_vector_db(embedding_model:Any,db_path:str)->Chroma:\n",
    "    if os.path.exists(db_path):\n",
    "        shutil.rmtree(db_path)\n",
    "    chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "    vector_db = Chroma(\n",
    "        collection_name='rag_app_collection',\n",
    "        embedding_function=embedding_model,\n",
    "        persist_directory=db_path \n",
    "        )\n",
    "    return vector_db\n",
    "\n",
    "def get_context_from_db(vector_db:VectorStore, query:str, k:int=5, score_threshold:float=None)->List[Document]:\n",
    "    contexts = vector_db.similarity_search_with_relevance_scores(query,\n",
    "                                                                 k=k,\n",
    "                                                                 score_threshold=score_threshold)\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要情報をフォーマットしてpromptに付け加える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※rag.pyに追加（app.pyにあった`format_prompt()`は消す）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(contexts:List[Document], query:str, chat_history:List[Dict[str, str]])->str:\n",
    "    PROMPT = \"\"\"\n",
    "    You are a helpful assistant. Answer the following questions based on the given context:\n",
    "    chat history: {CHAT_HISTORY}\n",
    "\n",
    "    context: {CONTEXT}\n",
    "\n",
    "    Answer the following questions based on the given context:\n",
    "    query: {QUERY}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(PROMPT)\n",
    "    chat_history = '\\n\\n'.join([f\"{message['role']}: {message['content']}\" for message in chat_history])\n",
    "    sources = [ {'source':doc[0].metadata['source'],'page':doc[0].metadata['page']} for doc in contexts ]\n",
    "    contexts = '\\n'.join([f\"CONTEXT {idx}:\\n{res.page_content}\" for idx, (res, _score) in enumerate(contexts)])\n",
    "    prompt = prompt.format(CHAT_HISTORY=chat_history,CONTEXT=contexts, QUERY=query)\n",
    "    return prompt, sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chat機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル入力にcontext情報を加える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
